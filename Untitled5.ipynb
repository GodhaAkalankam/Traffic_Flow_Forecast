{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d352d44-4957-401b-8715-800853d323bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed65bf0-597f-487b-bdaf-f4cab50bf8d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15a1fc3d-bc4a-4c2c-b416-d1ea1c544db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Processing the data\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "def process_data(train, test, lags):\n",
    "    \"\"\"Process data\n",
    "    Reshape and split train\\test data.\n",
    "\n",
    "    # Arguments\n",
    "        train: String, name of .csv train file.\n",
    "        test: String, name of .csv test file.\n",
    "        lags: integer, time lag.\n",
    "    # Returns\n",
    "        X_train: ndarray.\n",
    "        y_train: ndarray.\n",
    "        X_test: ndarray.\n",
    "        y_test: ndarray.\n",
    "        scaler: StandardScaler.\n",
    "    \"\"\"\n",
    "    attr = 'Lane 1 Flow (Veh/5 Minutes)'\n",
    "    df1 = pd.read_csv(train, encoding='utf-8').fillna(0)\n",
    "    df2 = pd.read_csv(test, encoding='utf-8').fillna(0)\n",
    "\n",
    "    # scaler = StandardScaler().fit(df1[attr].values)\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1)).fit(df1[attr].values.reshape(-1, 1))\n",
    "    flow1 = scaler.transform(df1[attr].values.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "    flow2 = scaler.transform(df2[attr].values.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "\n",
    "    train, test = [], []\n",
    "    for i in range(lags, len(flow1)):\n",
    "        train.append(flow1[i - lags: i + 1])\n",
    "    for i in range(lags, len(flow2)):\n",
    "        test.append(flow2[i - lags: i + 1])\n",
    "\n",
    "    train = np.array(train)\n",
    "    test = np.array(test)\n",
    "    np.random.shuffle(train)\n",
    "\n",
    "    X_train = train[:, :-1]\n",
    "    y_train = train[:, -1]\n",
    "    X_test = test[:, :-1]\n",
    "    y_test = test[:, -1]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "598f865c-d30a-41e7-8773-e6a419c967d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Defination of NN model\n",
    "\"\"\"\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "def get_lstm(units):\n",
    "    \"\"\"LSTM(Long Short-Term Memory)\n",
    "    Build LSTM Model.\n",
    "\n",
    "    # Arguments\n",
    "        units: List(int), number of input, output and hidden units.\n",
    "    # Returns\n",
    "        model: Model, nn model.\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units[1], input_shape=(units[0], 1), return_sequences=True))\n",
    "    model.add(LSTM(units[2]))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units[3], activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_gru(units):\n",
    "    \"\"\"GRU(Gated Recurrent Unit)\n",
    "    Build GRU Model.\n",
    "\n",
    "    # Arguments\n",
    "        units: List(int), number of input, output and hidden units.\n",
    "    # Returns\n",
    "        model: Model, nn model.\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units[1], input_shape=(units[0], 1), return_sequences=True))\n",
    "    model.add(GRU(units[2]))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units[3], activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def _get_sae(inputs, hidden, output):\n",
    "    \"\"\"SAE(Auto-Encoders)\n",
    "    Build SAE Model.\n",
    "\n",
    "    # Arguments\n",
    "        inputs: Integer, number of input units.\n",
    "        hidden: Integer, number of hidden units.\n",
    "        output: Integer, number of output units.\n",
    "    # Returns\n",
    "        model: Model, nn model.\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden, input_dim=inputs, name='hidden'))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(output, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_saes(layers):\n",
    "    \"\"\"SAEs(Stacked Auto-Encoders)\n",
    "    Build SAEs Model.\n",
    "\n",
    "    # Arguments\n",
    "        layers: List(int), number of input, output and hidden units.\n",
    "    # Returns\n",
    "        models: List(Model), List of SAE and SAEs.\n",
    "    \"\"\"\n",
    "    sae1 = _get_sae(layers[0], layers[1], layers[-1])\n",
    "    sae2 = _get_sae(layers[1], layers[2], layers[-1])\n",
    "    sae3 = _get_sae(layers[2], layers[3], layers[-1])\n",
    "\n",
    "    saes = Sequential()\n",
    "    saes.add(Dense(layers[1], input_dim=layers[0], name='hidden1'))\n",
    "    saes.add(Activation('sigmoid'))\n",
    "    saes.add(Dense(layers[2], name='hidden2'))\n",
    "    saes.add(Activation('sigmoid'))\n",
    "    saes.add(Dense(layers[3], name='hidden3'))\n",
    "    saes.add(Activation('sigmoid'))\n",
    "    saes.add(Dropout(0.2))\n",
    "    saes.add(Dense(layers[4], activation='sigmoid'))\n",
    "\n",
    "    models = [sae1, sae2, sae3, saes]\n",
    "\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a170446-ba97-4cb9-86ec-3c10e0387b23",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "GRU(reset_after=False) is not compatible with GRU(reset_after=True)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 127\u001b[0m\n\u001b[0;32m    123\u001b[0m     plot_results(y_test[: \u001b[38;5;241m288\u001b[39m], y_preds, names)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 127\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 98\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m     97\u001b[0m     lstm \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel/lstm.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m     gru \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel/gru.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m     saes \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel/saes.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    100\u001b[0m     models \u001b[38;5;241m=\u001b[39m [lstm, gru, saes]\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:262\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    255\u001b[0m         filepath,\n\u001b[0;32m    256\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    258\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    259\u001b[0m     )\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[1;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_sm_saving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\saving\\legacy\\hdf5_format.py:662\u001b[0m, in \u001b[0;36m_convert_rnn_weights\u001b[1;34m(layer, weights)\u001b[0m\n\u001b[0;32m    660\u001b[0m types \u001b[38;5;241m=\u001b[39m (source, target)\n\u001b[0;32m    661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGRU(reset_after=False)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m types:\n\u001b[1;32m--> 662\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not compatible with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m types)\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCuDNNGRU\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    664\u001b[0m     weights \u001b[38;5;241m=\u001b[39m convert_gru_weights(weights, from_cudnn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: GRU(reset_after=False) is not compatible with GRU(reset_after=True)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Traffic Flow Prediction with Neural Networks(SAEs、LSTM、GRU).\n",
    "\"\"\"\n",
    "import math\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from data.data import process_data\n",
    "from keras.models import load_model\n",
    "from keras.utils import plot_model\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def MAPE(y_true, y_pred):\n",
    "    \"\"\"Mean Absolute Percentage Error\n",
    "    Calculate the mape.\n",
    "\n",
    "    # Arguments\n",
    "        y_true: List/ndarray, ture data.\n",
    "        y_pred: List/ndarray, predicted data.\n",
    "    # Returns\n",
    "        mape: Double, result data for train.\n",
    "    \"\"\"\n",
    "\n",
    "    y = [x for x in y_true if x > 0]\n",
    "    y_pred = [y_pred[i] for i in range(len(y_true)) if y_true[i] > 0]\n",
    "\n",
    "    num = len(y_pred)\n",
    "    sums = 0\n",
    "\n",
    "    for i in range(num):\n",
    "        tmp = abs(y[i] - y_pred[i]) / y[i]\n",
    "        sums += tmp\n",
    "\n",
    "    mape = sums * (100 / num)\n",
    "\n",
    "    return mape\n",
    "\n",
    "\n",
    "def eva_regress(y_true, y_pred):\n",
    "    \"\"\"Evaluation\n",
    "    evaluate the predicted resul.\n",
    "\n",
    "    # Arguments\n",
    "        y_true: List/ndarray, ture data.\n",
    "        y_pred: List/ndarray, predicted data.\n",
    "    \"\"\"\n",
    "\n",
    "    mape = MAPE(y_true, y_pred)\n",
    "    vs = metrics.explained_variance_score(y_true, y_pred)\n",
    "    mae = metrics.mean_absolute_error(y_true, y_pred)\n",
    "    mse = metrics.mean_squared_error(y_true, y_pred)\n",
    "    r2 = metrics.r2_score(y_true, y_pred)\n",
    "    print('explained_variance_score:%f' % vs)\n",
    "    print('mape:%f%%' % mape)\n",
    "    print('mae:%f' % mae)\n",
    "    print('mse:%f' % mse)\n",
    "    print('rmse:%f' % math.sqrt(mse))\n",
    "    print('r2:%f' % r2)\n",
    "\n",
    "\n",
    "def plot_results(y_true, y_preds, names):\n",
    "    \"\"\"Plot\n",
    "    Plot the true data and predicted data.\n",
    "\n",
    "    # Arguments\n",
    "        y_true: List/ndarray, ture data.\n",
    "        y_pred: List/ndarray, predicted data.\n",
    "        names: List, Method names.\n",
    "    \"\"\"\n",
    "    d = '2016-3-4 00:00'\n",
    "    x = pd.date_range(d, periods=288, freq='5min')\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ax.plot(x, y_true, label='True Data')\n",
    "    for name, y_pred in zip(names, y_preds):\n",
    "        ax.plot(x, y_pred, label=name)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('Time of Day')\n",
    "    plt.ylabel('Flow')\n",
    "\n",
    "    date_format = mpl.dates.DateFormatter(\"%H:%M\")\n",
    "    ax.xaxis.set_major_formatter(date_format)\n",
    "    fig.autofmt_xdate()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    lstm = load_model('model/lstm.h5')\n",
    "    gru = load_model('model/gru.h5')\n",
    "    saes = load_model('model/saes.h5')\n",
    "    models = [lstm, gru, saes]\n",
    "    names = ['LSTM', 'GRU', 'SAEs']\n",
    "\n",
    "    lag = 12\n",
    "    file1 = 'data/train.csv'\n",
    "    file2 = 'data/test.csv'\n",
    "    _, _, X_test, y_test, scaler = process_data(file1, file2, lag)\n",
    "    y_test = scaler.inverse_transform(y_test.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "\n",
    "    y_preds = []\n",
    "    for name, model in zip(names, models):\n",
    "        if name == 'SAEs':\n",
    "            X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1]))\n",
    "        else:\n",
    "            X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "        file = 'images/' + name + '.png'\n",
    "        plot_model(model, to_file=file, show_shapes=True)\n",
    "        predicted = model.predict(X_test)\n",
    "        predicted = scaler.inverse_transform(predicted.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "        y_preds.append(predicted[:288])\n",
    "        print(name)\n",
    "        eva_regress(y_test, predicted)\n",
    "\n",
    "    plot_results(y_test[: 288], y_preds, names)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f800521-f7b9-4bf9-af81-d7b70c8c04ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4868725a-d945-4861-937e-c2988bcbe1a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
